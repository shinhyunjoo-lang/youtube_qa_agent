import os
import json
from typing import Optional
from datetime import datetime
from langchain_community.document_loaders import YoutubeLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.tools import tool
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.documents import Document

try:
    from langchain.chains.summarize import load_summarize_chain
    from langchain.chains import RetrievalQA
except ImportError:
    from langchain_classic.chains.summarize import load_summarize_chain
    from langchain_classic.chains import RetrievalQA

# Global state for agent (will be set by YouTubeAgent instance)
_agent_state = {
    "llm": None,
    "docs": [],
    "vector_store": None,
    "qa_chain": None,
    "search": None,
    "video_context": "",
    "conversation_memory": {},
    "video_info": {}
}

# ============================================================================
# TOOL DEFINITIONS using @tool decorator
# ============================================================================

@tool
def summarize_video(query: str = "") -> str:
    """Summarizes the entire YouTube video content. Use this when user asks for a general summary or overview."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        print(f"ğŸ“ [TOOL] ë¹„ë””ì˜¤ ìš”ì•½ ìƒì„± ì¤‘...")
        
        # Create a Korean-specific summarization prompt
        korean_prompt = PromptTemplate(
            template="ë‹¤ìŒ ë¹„ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ê³¼ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í¬í•¨í•´ì„œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”:\n\n{text}",
            input_variables=["text"]
        )
        
        chain = load_summarize_chain(_agent_state["llm"], chain_type="map_reduce", map_prompt=korean_prompt, combine_prompt=korean_prompt)
        return chain.run(_agent_state["docs"])
    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@tool
def generate_titles(query: str = "") -> str:
    """Generates catchy titles for the video. Use when user asks for title suggestions."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    print(f"ğŸ¯ [TOOL] ì œëª© ìƒì„± ì¤‘...")
    
    summary = summarize_video.invoke("")
    prompt = PromptTemplate(
        template="ë‹¤ìŒ ë¹„ë””ì˜¤ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ë§¤ë ¥ì ì¸ í•œêµ­ì–´ ì œëª© 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{summary}",
        input_variables=["summary"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(summary)

@tool
def write_blog_post(query: str = "") -> str:
    """Writes a blog post based on the video content. Use when user asks for a blog post or article."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    print(f"âœï¸ [TOOL] ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì‘ì„± ì¤‘...")
    
    summary = summarize_video.invoke("")
    prompt = PromptTemplate(
        template="ë‹¤ìŒ ë¹„ë””ì˜¤ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì„œë¡ , ë³¸ë¡ , ê²°ë¡  êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n{summary}",
        input_variables=["summary"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(summary)

@tool
def generate_quiz(query: str = "") -> str:
    """Generates a multiple choice quiz based on video content. Use when user asks for a quiz or test."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    print(f"â“ [TOOL] í€´ì¦ˆ ìƒì„± ì¤‘...")
    
    summary = summarize_video.invoke("")
    prompt = PromptTemplate(
        template="ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ 5ë¬¸í•­ì˜ ê°ê´€ì‹ í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê° ë¬¸í•­ë§ˆë‹¤ 4ê°œì˜ ì„ íƒì§€ì™€ ì •ë‹µì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n\n{summary}",
        input_variables=["summary"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(summary)

@tool
def extract_key_moments(query: str = "") -> str:
    """Extracts key moments, topics, and takeaways from the video. Use when user asks for highlights or main points."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    print(f"â­ [TOOL] í•µì‹¬ ìˆœê°„ ì¶”ì¶œ ì¤‘...")
    
    # Use the full transcript instead of summary for better timestamp extraction
    full_content = _agent_state["docs"][0].page_content if _agent_state["docs"] else ""
    
    prompt = PromptTemplate(
        template="""ë‹¤ìŒ ë¹„ë””ì˜¤ ì „ì²´ ë‚´ìš©ì—ì„œ í•µì‹¬ ìˆœê°„ë“¤ê³¼ ì£¼ìš” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. 
ê° í•µì‹¬ ìˆœê°„ë§ˆë‹¤ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ "0:30", "1:45", "3:20" í˜•ì‹ìœ¼ë¡œ í¬í•¨í•˜ì—¬ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ë¹„ë””ì˜¤ ë‚´ìš©:
{content}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
**0:00 - ì‹œì‘ ë¶€ë¶„**
- ì£¼ìš” ë‚´ìš© ì„¤ëª…

**1:30 - ì¤‘ê°„ ë¶€ë¶„**  
- ì£¼ìš” ë‚´ìš© ì„¤ëª…

**3:45 - ë§ˆë¬´ë¦¬ ë¶€ë¶„**
- ì£¼ìš” ë‚´ìš© ì„¤ëª…

í•µì‹¬ ìˆœê°„ë“¤:""",
        input_variables=["content"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(full_content[:4000])  # Limit content length for better processing

@tool
def search_web(query: str) -> str:
    """Searches the web for information NOT in the video. Use for current events, speaker background, or external facts."""
    if not _agent_state["search"]:
        return "ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        print(f"ğŸ” [TOOL] ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
        
        # Get comprehensive video context
        video_context = _agent_state.get("video_context", "")
        conversation_memory = _agent_state.get("conversation_memory", {})
        video_info = _agent_state.get("video_info", {})
        docs = _agent_state.get("docs", [])
        
        # Extract detailed context from video content
        search_context = []
        
        # 1. Get video title and author from video_info
        video_title = video_info.get("title", "")
        video_author = video_info.get("author", "")
        
        if video_title and video_title != "YouTube Video":
            search_context.append(video_title)
        if video_author:
            search_context.append(video_author)
        
        # 2. Add stored memory context (channel, speaker, event info)
        memory_context = []
        if conversation_memory:
            for key, value in conversation_memory.items():
                memory_context.append(f"{key}: {value}")
                search_context.append(str(value))
        
        # 3. Extract key entities from video content using LLM
        video_entities = []
        if docs and len(docs) > 0:
            content_sample = docs[0].page_content[:2000]  # Use more content for better context
            
            entity_extraction_prompt = f"""ë‹¤ìŒ ë¹„ë””ì˜¤ ë‚´ìš©ì—ì„œ ì›¹ ê²€ìƒ‰ì— ìœ ìš©í•œ í•µì‹¬ ì—”í‹°í‹°ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¶”ì¶œí•˜ì„¸ìš”:
- íšŒì‚¬ëª…/ì¡°ì§ëª…
- ì œí’ˆëª…/ì„œë¹„ìŠ¤ëª…  
- ê¸°ìˆ ëª…/í”Œë«í¼ëª…
- ì¸ë¬¼ëª…
- ì´ë²¤íŠ¸ëª…/ì»¨í¼ëŸ°ìŠ¤ëª…
- ì£¼ìš” í‚¤ì›Œë“œ

ë¹„ë””ì˜¤ ë‚´ìš©:
{content_sample}

ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì°¾ì€ ì—”í‹°í‹°ë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”. ì—†ìœ¼ë©´ "ì—†ìŒ"ì´ë¼ê³  í•˜ì„¸ìš”.

íšŒì‚¬ëª…/ì¡°ì§ëª…:
ì œí’ˆëª…/ì„œë¹„ìŠ¤ëª…:
ê¸°ìˆ ëª…/í”Œë«í¼ëª…:
ì¸ë¬¼ëª…:
ì´ë²¤íŠ¸ëª…/ì»¨í¼ëŸ°ìŠ¤ëª…:
ì£¼ìš” í‚¤ì›Œë“œ:"""
            
            try:
                entity_response = _agent_state["llm"].invoke([HumanMessage(content=entity_extraction_prompt)])
                entity_text = entity_response.content.strip()
                
                # Parse extracted entities
                for line in entity_text.split('\n'):
                    if ':' in line and 'ì—†ìŒ' not in line.lower():
                        entities = line.split(':', 1)[1].strip()
                        if entities and len(entities) > 3:
                            video_entities.extend([e.strip() for e in entities.split(',') if e.strip()])
                
                # Add top entities to search context
                search_context.extend(video_entities[:5])  # Top 5 entities
                
            except Exception as e:
                print(f"Entity extraction error: {e}")
        
        # 4. Create intelligent search query
        if not search_context:
            return "ğŸš« ê²€ìƒ‰í•  êµ¬ì²´ì ì¸ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë¹„ë””ì˜¤ ë‚´ìš©ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # Build search query with context
        context_terms = " ".join(search_context[:4])  # Use top 4 context terms
        
        # Smart query construction based on user intent
        if any(word in query.lower() for word in ['ê´€ë ¨', 'ë”', 'ìì„¸í•œ', 'ì„¤ëª…', 'ì •ë³´']):
            # User wants more detailed information about the video topic
            final_query = f"{context_terms} ìì„¸í•œ ì„¤ëª… ìµœì‹  ì •ë³´"
        elif any(word in query.lower() for word in ['ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ì†Œì‹']):
            # User wants news/articles
            final_query = f"{context_terms} ë‰´ìŠ¤ ê¸°ì‚¬ ìµœì‹ "
        elif any(word in query.lower() for word in ['ë°°ê²½', 'ì—­ì‚¬', 'ì†Œê°œ']):
            # User wants background information
            final_query = f"{context_terms} ë°°ê²½ ì†Œê°œ ê°œìš”"
        else:
            # General search with context
            final_query = f"{query} {context_terms}"
            
        print(f"ğŸ” ê²€ìƒ‰ì–´: {final_query}")
        print(f"ğŸ“‹ ì¶”ì¶œëœ ì»¨í…ìŠ¤íŠ¸: {', '.join(search_context[:5])}")
        
        # Perform web search
        search_result = _agent_state["search"].invoke(final_query)
        
        if not search_result or len(search_result.strip()) < 50:
            return "ğŸš« ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”."
        
        # Enhanced result filtering with video context
        detailed_info_available = video_info.get('detailed_info_available', False)
        
        filter_prompt = f"""ë‹¤ìŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ í˜„ì¬ ë¡œë“œëœ ë¹„ë””ì˜¤ì™€ ê´€ë ¨ëœ ì •ë³´ë§Œ ì„ ë³„í•´ì£¼ì„¸ìš”.

í˜„ì¬ ë¹„ë””ì˜¤ ì •ë³´:
- ì œëª©: {video_title}"""
        
        if detailed_info_available:
            filter_prompt += f"""
- ì±„ë„/ì‘ì„±ì: {video_author}
- ì¡°íšŒìˆ˜: {video_info.get('view_count', 'ì •ë³´ ì—†ìŒ')}
- ê¸¸ì´: {video_info.get('length', 'ì •ë³´ ì—†ìŒ')}
- ê²Œì‹œì¼: {video_info.get('publish_date', 'ì •ë³´ ì—†ìŒ')}
- ì„¤ëª…: {video_info.get('description', '')[:200]}..."""
        else:
            filter_prompt += "\n- ìƒì„¸ ì •ë³´: ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš© ê°€ëŠ¥"
            
        filter_prompt += f"""
- ì €ì¥ëœ ì •ë³´: {json.dumps(conversation_memory, ensure_ascii=False)}
- ì¶”ì¶œëœ ì£¼ìš” ì—”í‹°í‹°: {', '.join(video_entities[:10])}
- ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ ê²°ê³¼:
{search_result}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì •ë³´ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”:
1. ìœ„ ë¹„ë””ì˜¤ ì •ë³´ì™€ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ì„ ë³„
2. ë¹„ë””ì˜¤ì—ì„œ ì–¸ê¸‰ëœ íšŒì‚¬, ì œí’ˆ, ê¸°ìˆ , ì¸ë¬¼, ì´ë²¤íŠ¸ì™€ ê´€ë ¨ëœ ì •ë³´ ìš°ì„ 
3. ìµœì‹  ì •ë³´ ë° ê³µì‹ ë°œí‘œ ë‚´ìš© í¬í•¨
4. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ìš”ì•½
5. ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì ì¸ ì •ë³´ë‚˜ ê´‘ê³ ëŠ” ì œì™¸
6. ì¶œì²˜ë‚˜ ë‚ ì§œê°€ ìˆë‹¤ë©´ í¬í•¨

ë¹„ë””ì˜¤ì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì •ë³´:"""
        
        response = _agent_state["llm"].invoke([HumanMessage(content=filter_prompt)])
        result = response.content.strip()
        
        if len(result) < 50 or "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†" in result:
            return f"ğŸš« '{video_title}'ê³¼ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì›¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ë³´ì„¸ìš”."
            
        return f"ğŸŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ('{video_title}' ê´€ë ¨):\n\n{result}"
        
    except Exception as e:
        print(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@tool
def store_memory(user_message: str) -> str:
    """Stores factual information provided by the user about the video (e.g., channel name, speaker name).
    ONLY use when user provides NEW facts as STATEMENTS, NOT for questions."""
    try:
        print(f"ğŸ’¾ [TOOL] ë©”ëª¨ë¦¬ ì €ì¥ ì¤‘...")
        
        extraction_prompt = f"""Analyze the following user message and extract any factual information they are providing about the video.
Return ONLY a JSON object with key-value pairs. If no factual information is provided, return {{}}.

Examples:
- "ì´ ë¹„ë””ì˜¤ëŠ” AWS Events ì±„ë„ì— ì˜¬ë¼ì™”ì–´" â†’ {{"channel": "AWS Events"}}
- "ë°œí‘œìëŠ” Johnì´ì•¼" â†’ {{"speaker": "John"}}
- "ì´ê±´ re:Invent 2024 ì„¸ì…˜ì´ì•¼" â†’ {{"event": "re:Invent 2024"}}

User message: {user_message}

Return JSON:"""
        
        response = _agent_state["llm"].invoke([HumanMessage(content=extraction_prompt)])
        content = response.content.strip()
        
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].strip()
        
        extracted_data = json.loads(content)
        
        if extracted_data:
            # 1. ë©”ëª¨ë¦¬ì— ì €ì¥ (ê¸°ì¡´ ë°©ì‹)
            _agent_state["conversation_memory"].update(extracted_data)
            
            # 2. ë²¡í„° ìŠ¤í† ì–´ì—ë„ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì¶”ê°€
            if _agent_state["vector_store"]:
                memory_docs = []
                for key, value in extracted_data.items():
                    # ìì—°ì–´ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„°í™”
                    if key == "channel":
                        doc_content = f"ì´ ë¹„ë””ì˜¤ëŠ” {value} ì±„ë„ì—ì„œ ì œê³µë©ë‹ˆë‹¤."
                    elif key == "speaker":
                        doc_content = f"ì´ ë¹„ë””ì˜¤ì˜ ë°œí‘œìëŠ” {value}ì…ë‹ˆë‹¤."
                    elif key == "event":
                        doc_content = f"ì´ ë¹„ë””ì˜¤ëŠ” {value} ì´ë²¤íŠ¸ì˜ ì¼ë¶€ì…ë‹ˆë‹¤."
                    else:
                        doc_content = f"ì´ ë¹„ë””ì˜¤ì˜ {key}ëŠ” {value}ì…ë‹ˆë‹¤."
                    
                    memory_doc = Document(
                        page_content=doc_content,
                        metadata={"type": "memory", "key": key, "value": value}
                    )
                    memory_docs.append(memory_doc)
                
                # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
                _agent_state["vector_store"].add_documents(memory_docs)
                print(f"DEBUG: Added {len(memory_docs)} memory documents to vector store")
            
            stored_keys = ", ".join(extracted_data.keys())
            return f"âœ“ ì •ë³´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {stored_keys}"
        else:
            return "ì €ì¥í•  ìƒˆë¡œìš´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        return f"ë©”ëª¨ë¦¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}"

@tool
def answer_question(question: str) -> str:
    """Answers specific questions about the video content using the transcript. 
    Use for detailed questions about what was said in the video."""
    if not _agent_state["qa_chain"]:
        return "ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ë¥¼ ë‹¤ì‹œ ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    try:
        print(f"ğŸ¤” [TOOL] ë¹„ë””ì˜¤ ë‚´ìš© ì§ˆë¬¸ ë‹µë³€ ì¤‘...")
        
        # Create a Korean-specific QA prompt
        korean_qa_prompt = PromptTemplate(
            template="""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:""",
            input_variables=["context", "question"]
        )
        
        # Update QA chain with Korean prompt
        try:
            from langchain.chains import RetrievalQA
        except ImportError:
            from langchain_classic.chains import RetrievalQA
            
        qa_chain = RetrievalQA.from_chain_type(
            llm=_agent_state["llm"],
            chain_type="stuff",
            retriever=_agent_state["vector_store"].as_retriever(),
            chain_type_kwargs={"prompt": korean_qa_prompt}
        )
        
        return qa_chain.run(question)
    except Exception as e:
        return f"ì§ˆë¬¸ ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ============================================================================
# YOUTUBE AGENT CLASS
# ============================================================================

class YouTubeAgent:
    def __init__(self, openai_api_key):
        self.openai_api_key = openai_api_key
        os.environ["OPENAI_API_KEY"] = openai_api_key
        self.llm = ChatOpenAI(temperature=0.2, model_name="gpt-4o-mini")
        self.docs = []
        self.vector_store = None
        self.qa_chain = None
        self.search = DuckDuckGoSearchRun()
        self.video_context = ""
        self.conversation_memory = {}
        self.video_info = {}
        self.video_id = ""
        
        # Update global state
        _agent_state["llm"] = self.llm
        _agent_state["search"] = self.search
        
        # Create tools mapping for simple routing
        self.tools = {
            "summarize": summarize_video,
            "titles": generate_titles,
            "blog": write_blog_post,
            "quiz": generate_quiz,
            "moments": extract_key_moments,
            "search": search_web,
            "memory": store_memory,
            "answer": answer_question
        }
    
    def save_metadata(self):
        """Save conversation memory, context, and video info to JSON file."""
        try:
            db_path = f"db/{self.video_id}"
            os.makedirs(db_path, exist_ok=True)
            
            # Extract video info from docs metadata if available
            video_info = {}
            if self.docs and len(self.docs) > 0:
                doc_metadata = self.docs[0].metadata
                video_info = {
                    "title": doc_metadata.get("title", ""),
                    "description": doc_metadata.get("description", ""),
                    "view_count": doc_metadata.get("view_count", ""),
                    "length": doc_metadata.get("length", ""),
                    "author": doc_metadata.get("author", ""),
                    "publish_date": doc_metadata.get("publish_date", ""),
                    "upload_date": doc_metadata.get("upload_date", "")
                }
            
            metadata = {
                "conversation_memory": self.conversation_memory,
                "video_context": self.video_context,
                "video_info": video_info,
                "timestamp": datetime.now().isoformat(),
                "video_id": self.video_id
            }
            
            metadata_path = f"{db_path}/metadata.json"
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"DEBUG: Saved metadata with video info to {metadata_path}")
        except Exception as e:
            print(f"Warning: Could not save metadata: {e}")
    
    def load_metadata(self):
        """Load conversation memory, context, and video info from JSON file."""
        try:
            metadata_path = f"db/{self.video_id}/metadata.json"
            if os.path.exists(metadata_path):
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                
                self.conversation_memory = metadata.get("conversation_memory", {})
                saved_context = metadata.get("video_context", "")
                self.video_info = metadata.get("video_info", {})
                
                # ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ì™€ ë³‘í•©
                if saved_context and saved_context != self.video_context:
                    self.video_context = saved_context
                
                print(f"DEBUG: Loaded metadata from {metadata_path}")
                print(f"DEBUG: Loaded conversation memory: {self.conversation_memory}")
                print(f"DEBUG: Loaded video info: {self.video_info}")
                
                # ê¸€ë¡œë²Œ ìƒíƒœ ì—…ë°ì´íŠ¸
                _agent_state["conversation_memory"] = self.conversation_memory
                _agent_state["video_context"] = self.video_context
                _agent_state["video_info"] = getattr(self, 'video_info', {})
                
                return True
        except Exception as e:
            print(f"Warning: Could not load metadata: {e}")
        
        return False
    
    def add_video_info_to_vector_store(self, video_info):
        """Add video information to vector store for semantic search."""
        if self.vector_store and video_info:
            try:
                video_docs = []
                
                # Add title
                if video_info.get("title"):
                    title_doc = Document(
                        page_content=f"ë¹„ë””ì˜¤ ì œëª©: {video_info['title']}",
                        metadata={"type": "video_info", "info_type": "title"}
                    )
                    video_docs.append(title_doc)
                
                # Add description
                if video_info.get("description"):
                    desc_doc = Document(
                        page_content=f"ë¹„ë””ì˜¤ ì„¤ëª…: {video_info['description'][:500]}",  # Limit description length
                        metadata={"type": "video_info", "info_type": "description"}
                    )
                    video_docs.append(desc_doc)
                
                # Add author/channel info
                if video_info.get("author"):
                    author_doc = Document(
                        page_content=f"ë¹„ë””ì˜¤ ì±„ë„/ì‘ì„±ì: {video_info['author']}",
                        metadata={"type": "video_info", "info_type": "author"}
                    )
                    video_docs.append(author_doc)
                
                # Add view count and length info
                stats_info = []
                if video_info.get("view_count"):
                    stats_info.append(f"ì¡°íšŒìˆ˜: {video_info['view_count']}")
                if video_info.get("length"):
                    stats_info.append(f"ê¸¸ì´: {video_info['length']}")
                if video_info.get("publish_date"):
                    stats_info.append(f"ê²Œì‹œì¼: {video_info['publish_date']}")
                
                if stats_info:
                    stats_doc = Document(
                        page_content=f"ë¹„ë””ì˜¤ ì •ë³´: {', '.join(stats_info)}",
                        metadata={"type": "video_info", "info_type": "stats"}
                    )
                    video_docs.append(stats_doc)
                
                if video_docs:
                    self.vector_store.add_documents(video_docs)
                    print(f"DEBUG: Added {len(video_docs)} video info documents to vector store")
                    
            except Exception as e:
                print(f"Warning: Could not add video info to vector store: {e}")
    
    def load_video(self, url):
        """Loads the video transcript with detailed video information."""
        try:
            self.docs = []
            self.vector_store = None
            self.qa_chain = None
            self.video_context = ""
            self.video_info = {}

            # First try with detailed info, fallback to basic if it fails
            try:
                print("ğŸ”„ ë¹„ë””ì˜¤ ìƒì„¸ ì •ë³´ì™€ í•¨ê»˜ ë¡œë“œ ì‹œë„ ì¤‘...")
                loader = YoutubeLoader.from_youtube_url(url, add_video_info=True, language=["en", "en-US", "ko"])
                self.docs = loader.load()
                detailed_info_loaded = True
                print("âœ… ìƒì„¸ ì •ë³´ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ ìƒì„¸ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ğŸ”„ ê¸°ë³¸ ì •ë³´ë¡œ ì¬ì‹œë„ ì¤‘...")
                loader = YoutubeLoader.from_youtube_url(url, add_video_info=False, language=["en", "en-US", "ko"])
                self.docs = loader.load()
                detailed_info_loaded = False
                print("âœ… ê¸°ë³¸ ì •ë³´ ë¡œë“œ ì„±ê³µ")
            
            if not self.docs:
                return "ì˜¤ë¥˜: ì´ ë¹„ë””ì˜¤ì˜ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´ ìë§‰ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
            
            # Extract video information from metadata
            doc_metadata = self.docs[0].metadata
            print(f"DEBUG: Available metadata keys: {list(doc_metadata.keys())}")
            print(f"DEBUG: Metadata content: {doc_metadata}")
            
            # Extract basic info that should always be available
            title = doc_metadata.get('title', 'YouTube Video')
            source = doc_metadata.get('source', '')
            
            # Extract detailed info if available
            description = doc_metadata.get('description', '') if detailed_info_loaded else ''
            author = doc_metadata.get('author', '') if detailed_info_loaded else ''
            view_count = doc_metadata.get('view_count', '') if detailed_info_loaded else ''
            length = doc_metadata.get('length', '') if detailed_info_loaded else ''
            publish_date = doc_metadata.get('publish_date', '') if detailed_info_loaded else ''
            upload_date = doc_metadata.get('upload_date', '') if detailed_info_loaded else ''
            
            # If basic info failed, try to extract from source URL or other fields
            if title == 'YouTube Video' or not title:
                # Try to get title from other metadata fields
                for key in ['video_title', 'name', 'display_name']:
                    if key in doc_metadata and doc_metadata[key]:
                        title = doc_metadata[key]
                        break
                
                # If still no title, extract video ID from URL for identification
                if title == 'YouTube Video' or not title:
                    if "v=" in url:
                        video_id_from_url = url.split("v=")[1].split("&")[0]
                        title = f"YouTube Video ({video_id_from_url})"
                    elif "youtu.be/" in url:
                        video_id_from_url = url.split("youtu.be/")[1].split("?")[0]
                        title = f"YouTube Video ({video_id_from_url})"
            
            # Store video info
            self.video_info = {
                "title": title,
                "description": description,
                "author": author,
                "view_count": view_count,
                "length": length,
                "publish_date": publish_date,
                "upload_date": upload_date,
                "source": source,
                "detailed_info_available": detailed_info_loaded
            }
            
            print(f"DEBUG: Extracted video info: {self.video_info}")
            
            # Create video context based on available information
            context_parts = [f"Video Title: {title}"]
            if author:
                context_parts.append(f"Channel: {author}")
            if view_count:
                context_parts.append(f"Views: {view_count}")
            if length:
                context_parts.append(f"Length: {length}")
            if publish_date:
                context_parts.append(f"Published: {publish_date}")
            if description and len(description) > 0:
                # Add first 200 characters of description
                desc_preview = description[:200] + "..." if len(description) > 200 else description
                context_parts.append(f"Description: {desc_preview}")
            
            self.video_context = "\n".join(context_parts)
            
            # Extract video ID
            if "v=" in url:
                self.video_id = url.split("v=")[1].split("&")[0]
            elif "youtu.be/" in url:
                self.video_id = url.split("youtu.be/")[1].split("?")[0]
            else:
                import hashlib
                self.video_id = hashlib.md5(url.encode()).hexdigest()[:12]
            
            # Update global state
            _agent_state["docs"] = self.docs
            _agent_state["video_context"] = self.video_context
            _agent_state["video_info"] = self.video_info
            
            # Display loaded video info
            info_display = [f"ğŸ“º ë¹„ë””ì˜¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {title}"]
            
            if detailed_info_loaded:
                info_display.append("âœ… ìƒì„¸ ì •ë³´ í¬í•¨")
                if author:
                    info_display.append(f"ğŸ“º ì±„ë„: {author}")
                if view_count:
                    info_display.append(f"ğŸ‘€ ì¡°íšŒìˆ˜: {view_count}")
                if length:
                    info_display.append(f"â±ï¸ ê¸¸ì´: {length}")
                if publish_date:
                    info_display.append(f"ğŸ“… ê²Œì‹œì¼: {publish_date}")
            else:
                info_display.append("âš ï¸ ê¸°ë³¸ ì •ë³´ë§Œ ë¡œë“œë¨ (ìƒì„¸ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨)")
                
            return "\n".join(info_display)
            
        except Exception as e:
            return f"ë¹„ë””ì˜¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def create_vector_store(self):
        """Creates a FAISS vector store, using local persistence."""
        if not self.docs:
            return "ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        db_path = f"db/{self.video_id}"
        vector_store_loaded = False
        
        # 1. ë©”íƒ€ë°ì´í„° ë¡œë“œ (JSON)
        metadata_loaded = self.load_metadata()
        
        if os.path.exists(db_path):
            try:
                embeddings = OpenAIEmbeddings()
                self.vector_store = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
                print(f"DEBUG: Loaded existing vector store from {db_path}")
                vector_store_loaded = True
            except Exception as e:
                print(f"Warning: Could not load local index: {e}. Recreating.")
        
        if not self.vector_store:
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            split_docs = text_splitter.split_documents(self.docs)
            
            embeddings = OpenAIEmbeddings()
            self.vector_store = FAISS.from_documents(split_docs, embeddings)
            self.vector_store.save_local(db_path)
            print(f"DEBUG: Saved new vector store to {db_path}")
        
        # Create Korean QA chain
        korean_qa_prompt = PromptTemplate(
            template="""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:""",
            input_variables=["context", "question"]
        )
        
        try:
            from langchain.chains import RetrievalQA
        except ImportError:
            from langchain_classic.chains import RetrievalQA
            
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(),
            chain_type_kwargs={"prompt": korean_qa_prompt}
        )
        
        # 2. Add video information to vector store (for new vector stores)
        if not vector_store_loaded and hasattr(self, 'video_info') and self.video_info:
            # Only add detailed info if it was successfully loaded
            if self.video_info.get('detailed_info_available', False):
                self.add_video_info_to_vector_store(self.video_info)
            else:
                # Add basic title info to vector store
                if self.video_info.get('title'):
                    basic_info_doc = Document(
                        page_content=f"ë¹„ë””ì˜¤ ì œëª©: {self.video_info['title']}",
                        metadata={"type": "video_info", "info_type": "title"}
                    )
                    self.vector_store.add_documents([basic_info_doc])
                    print("DEBUG: Added basic video title to vector store")
        
        # 3. Generate and add context summary to vector store
        if "Summary:" not in self.video_context:
            try:
                brief_content = self.docs[0].page_content[:3000]
                summary_prompt = f"ë‹¤ìŒ ë¹„ë””ì˜¤ ë‚´ìš©ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{brief_content}"
                context_summary = self.llm.invoke([HumanMessage(content=summary_prompt)]).content
                self.video_context += f"\nSummary: {context_summary}"
                
                # Add context summary to vector store for semantic search
                self.add_context_to_vector_store(context_summary)
                print(f"DEBUG: Generated context summary")
            except Exception as e:
                print(f"Warning: Could not generate context summary: {e}")
        
        # 4. Add existing conversation memory to vector store (if loaded from JSON)
        if metadata_loaded and self.conversation_memory:
            try:
                memory_docs = []
                for key, value in self.conversation_memory.items():
                    if key == "channel":
                        doc_content = f"ì´ ë¹„ë””ì˜¤ëŠ” {value} ì±„ë„ì—ì„œ ì œê³µë©ë‹ˆë‹¤."
                    elif key == "speaker":
                        doc_content = f"ì´ ë¹„ë””ì˜¤ì˜ ë°œí‘œìëŠ” {value}ì…ë‹ˆë‹¤."
                    elif key == "event":
                        doc_content = f"ì´ ë¹„ë””ì˜¤ëŠ” {value} ì´ë²¤íŠ¸ì˜ ì¼ë¶€ì…ë‹ˆë‹¤."
                    else:
                        doc_content = f"ì´ ë¹„ë””ì˜¤ì˜ {key}ëŠ” {value}ì…ë‹ˆë‹¤."
                    
                    memory_doc = Document(
                        page_content=doc_content,
                        metadata={"type": "memory", "key": key, "value": value}
                    )
                    memory_docs.append(memory_doc)
                
                if memory_docs:
                    self.vector_store.add_documents(memory_docs)
                    print(f"DEBUG: Added {len(memory_docs)} existing memory documents to vector store")
            except Exception as e:
                print(f"Warning: Could not add existing memory to vector store: {e}")
        
        # Update global state
        _agent_state["vector_store"] = self.vector_store
        _agent_state["qa_chain"] = self.qa_chain
        _agent_state["video_context"] = self.video_context
        _agent_state["conversation_memory"] = self.conversation_memory
        
        # 5. Save metadata (JSON)
        self.save_metadata()
        
        # Return different messages based on whether vector store was loaded or created
        if vector_store_loaded:
            return "ğŸ“‚ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤."
        else:
            return "ğŸ”§ ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."

    def run(self, query):
        """Simple routing-based agent without complex agent framework."""
        if not self.docs:
            return "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        print(f"\nğŸ¯ ì‚¬ìš©ì ì§ˆë¬¸: {query}")
        
        # Pre-check: answer from memory if applicable
        if self.conversation_memory and any(keyword in query.lower() for keyword in ['ë­', 'ë¬´ì—‡', 'ì–´ë–¤', 'ëˆ„êµ¬', 'ì–´ë””', 'ì–¸ì œ', 'what', 'who', 'which', 'where', 'when', 'ì±„ë„']):
            try:
                print(f"ğŸ’¾ [ROUTE] ì €ì¥ëœ ë©”ëª¨ë¦¬ì—ì„œ ë‹µë³€ í™•ì¸ ì¤‘...")
                memory_check_prompt = f"""User has stored the following information:
{json.dumps(self.conversation_memory, ensure_ascii=False, indent=2)}

User question: {query}

If the question is asking about information that exists in the stored data above, answer it directly and concisely in Korean.
If the information is NOT in the stored data, respond with "NOT_FOUND".

Answer:"""
                
                response = self.llm.invoke([HumanMessage(content=memory_check_prompt)])
                answer = response.content.strip()
                
                if answer != "NOT_FOUND" and "NOT_FOUND" not in answer:
                    return f"ğŸ’¾ ì €ì¥ëœ ì •ë³´: {answer}"
            except Exception as e:
                print(f"Memory check error: {e}")
        
        # Update global state before processing
        _agent_state["conversation_memory"] = self.conversation_memory
        
        # Check if user is providing information (statements, not questions)
        if not any(q_word in query for q_word in ['?', 'ë­', 'ë¬´ì—‡', 'ì–´ë””', 'ëˆ„êµ¬', 'ì–¸ì œ', 'ì–´ë–»ê²Œ', 'what', 'who', 'where', 'when', 'how']):
            # This might be a statement providing information
            try:
                print(f"ğŸ’¾ [ROUTE] ì •ë³´ ì €ì¥ ì‹œë„ ì¤‘...")
                result = store_memory.invoke(query)
                if "ì •ë³´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤" in result:
                    # Update local memory as well
                    extraction_prompt = f"""Analyze the following user message and extract any factual information they are providing about the video.
Return ONLY a JSON object with key-value pairs. If no factual information is provided, return {{}}.

Examples:
- "ì´ ë¹„ë””ì˜¤ëŠ” AWS Events ì±„ë„ì— ì˜¬ë¼ì™”ì–´" â†’ {{"channel": "AWS Events"}}
- "ë°œí‘œìëŠ” Johnì´ì•¼" â†’ {{"speaker": "John"}}
- "ì´ê±´ re:Invent 2024 ì„¸ì…˜ì´ì•¼" â†’ {{"event": "re:Invent 2024"}}

User message: {query}

Return JSON:"""
                    
                    response = self.llm.invoke([HumanMessage(content=extraction_prompt)])
                    content = response.content.strip()
                    
                    if "```json" in content:
                        content = content.split("```json")[1].split("```")[0].strip()
                    elif "```" in content:
                        content = content.split("```")[1].strip()
                    
                    try:
                        extracted_data = json.loads(content)
                        if extracted_data:
                            self.conversation_memory.update(extracted_data)
                    except:
                        pass
                    
                    return result
            except Exception as e:
                print(f"Memory storage error: {e}")
        
        # Simple routing logic with tool selection display
        query_lower = query.lower()
        
        # Check for specific tool requests
        if any(word in query_lower for word in ['ìš”ì•½', 'summary', 'summarize']):
            print(f"ğŸ“ [ROUTE] ë¹„ë””ì˜¤ ìš”ì•½ ë„êµ¬ ì„ íƒ")
            return summarize_video.invoke("")
        elif any(word in query_lower for word in ['ì œëª©', 'title', 'titles']):
            print(f"ğŸ¯ [ROUTE] ì œëª© ìƒì„± ë„êµ¬ ì„ íƒ")
            return generate_titles.invoke("")
        elif any(word in query_lower for word in ['ë¸”ë¡œê·¸', 'blog', 'post']):
            print(f"âœï¸ [ROUTE] ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì‘ì„± ë„êµ¬ ì„ íƒ")
            return write_blog_post.invoke("")
        elif any(word in query_lower for word in ['í€´ì¦ˆ', 'quiz', 'test']):
            print(f"â“ [ROUTE] í€´ì¦ˆ ìƒì„± ë„êµ¬ ì„ íƒ")
            return generate_quiz.invoke("")
        elif any(word in query_lower for word in ['í•µì‹¬', 'ì¤‘ìš”', 'key', 'moments', 'highlights']):
            print(f"â­ [ROUTE] í•µì‹¬ ìˆœê°„ ì¶”ì¶œ ë„êµ¬ ì„ íƒ")
            return extract_key_moments.invoke("")
        elif any(word in query_lower for word in ['ê²€ìƒ‰', 'search', 'ì°¾ì•„', 'ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ê´€ë ¨', 'ì¶œì²˜']):
            print(f"ğŸ” [ROUTE] ì›¹ ê²€ìƒ‰ ë„êµ¬ ì„ íƒ")
            
            # If the query is asking for articles or sources, create a better search query
            if any(word in query_lower for word in ['ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ê´€ë ¨', 'ì¶œì²˜']):
                # Use the original query for search
                search_query = query
            else:
                # Extract search terms from query and enhance with video context
                search_query = query.replace('ê²€ìƒ‰', '').replace('ì°¾ì•„', '').replace('search', '').strip()
                
                # If no specific search terms after cleaning, use video context
                if not search_query or len(search_query) < 3:
                    if self.conversation_memory:
                        # Use stored information for search
                        search_terms = []
                        for key, value in self.conversation_memory.items():
                            search_terms.append(str(value))
                        search_query = " ".join(search_terms) + " ê´€ë ¨ ê¸°ì‚¬"
                    else:
                        # Use video title
                        title = self.docs[0].metadata.get('title', '') if self.docs else ''
                        search_query = f"{title} ê´€ë ¨ ê¸°ì‚¬" if title else "ê²€ìƒ‰í•  ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”."
            
            if search_query and search_query != "ê²€ìƒ‰í•  ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.":
                result = search_web.invoke(search_query)
                # Save metadata after any interaction
                self.save_metadata()
                return result
            else:
                return "ê²€ìƒ‰í•  ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”."
        else:
            # Default to answering questions about the video
            print(f"ğŸ¤” [ROUTE] ë¹„ë””ì˜¤ ì§ˆë¬¸ ë‹µë³€ ë„êµ¬ ì„ íƒ")
            result = answer_question.invoke(query)
            # Save metadata after any interaction
            self.save_metadata()
            return result
    def add_context_to_vector_store(self, summary):
        """Add context summary to vector store for semantic search."""
        if self.vector_store and summary:
            try:
                context_doc = Document(
                    page_content=f"ë¹„ë””ì˜¤ ìš”ì•½: {summary}",
                    metadata={"type": "context_summary"}
                )
                self.vector_store.add_documents([context_doc])
                print("DEBUG: Added context summary to vector store")
            except Exception as e:
                print(f"Warning: Could not add context to vector store: {e}")