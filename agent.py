import os
import json
from typing import Optional
from langchain_community.document_loaders import YoutubeLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.tools import tool
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage
from langchain_community.tools import DuckDuckGoSearchRun

try:
    from langchain.chains.summarize import load_summarize_chain
    from langchain.chains import RetrievalQA
except ImportError:
    from langchain_classic.chains.summarize import load_summarize_chain
    from langchain_classic.chains import RetrievalQA

# Global state for agent (will be set by YouTubeAgent instance)
_agent_state = {
    "llm": None,
    "docs": [],
    "vector_store": None,
    "qa_chain": None,
    "search": None,
    "video_context": "",
    "conversation_memory": {}
}

# ============================================================================
# TOOL DEFINITIONS using @tool decorator
# ============================================================================

@tool
def summarize_video(query: str = "") -> str:
    """Summarizes the entire YouTube video content. Use this when user asks for a general summary or overview."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        # Create a Korean-specific summarization prompt
        korean_prompt = PromptTemplate(
            template="ë‹¤ìŒ ë¹„ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ê³¼ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í¬í•¨í•´ì„œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”:\n\n{text}",
            input_variables=["text"]
        )
        
        chain = load_summarize_chain(_agent_state["llm"], chain_type="map_reduce", map_prompt=korean_prompt, combine_prompt=korean_prompt)
        return chain.run(_agent_state["docs"])
    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@tool
def generate_titles(query: str = "") -> str:
    """Generates catchy titles for the video. Use when user asks for title suggestions."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    summary = summarize_video.invoke("")
    prompt = PromptTemplate(
        template="ë‹¤ìŒ ë¹„ë””ì˜¤ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ë§¤ë ¥ì ì¸ í•œêµ­ì–´ ì œëª© 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{summary}",
        input_variables=["summary"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(summary)

@tool
def write_blog_post(query: str = "") -> str:
    """Writes a blog post based on the video content. Use when user asks for a blog post or article."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    summary = summarize_video.invoke("")
    prompt = PromptTemplate(
        template="ë‹¤ìŒ ë¹„ë””ì˜¤ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì„œë¡ , ë³¸ë¡ , ê²°ë¡  êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n{summary}",
        input_variables=["summary"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(summary)

@tool
def generate_quiz(query: str = "") -> str:
    """Generates a multiple choice quiz based on video content. Use when user asks for a quiz or test."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    summary = summarize_video.invoke("")
    prompt = PromptTemplate(
        template="ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ 5ë¬¸í•­ì˜ ê°ê´€ì‹ í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê° ë¬¸í•­ë§ˆë‹¤ 4ê°œì˜ ì„ íƒì§€ì™€ ì •ë‹µì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n\n{summary}",
        input_variables=["summary"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(summary)

@tool
def extract_key_moments(query: str = "") -> str:
    """Extracts key moments, topics, and takeaways from the video. Use when user asks for highlights or main points."""
    if not _agent_state["docs"]:
        return "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    summary = summarize_video.invoke("")
    prompt = PromptTemplate(
        template="ë‹¤ìŒ ë¹„ë””ì˜¤ì—ì„œ í•µì‹¬ ìˆœê°„ë“¤ê³¼ ì£¼ìš” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì¤‘ìš”í•œ í¬ì¸íŠ¸ë“¤ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:\n\n{summary}",
        input_variables=["summary"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(summary)

@tool
def translate_content(target_language: str = "Korean") -> str:
    """Translates the video summary into the specified language. Default is Korean."""
    if not _agent_state["docs"]:
        return "No video loaded."
    
    summary = summarize_video.invoke("")
    prompt = PromptTemplate(
        template=f"Translate the following text into {target_language}:\\n\\n{{text}}",
        input_variables=["text"]
    )
    try:
        from langchain.chains import LLMChain
    except ImportError:
        from langchain_classic.chains import LLMChain
    chain = LLMChain(llm=_agent_state["llm"], prompt=prompt)
    return chain.run(summary)

@tool
def search_web(query: str) -> str:
    """Searches the web for information NOT in the video. Use for current events, speaker background, or external facts. 
    IMPORTANT: Query must include specific entities/names from the video context."""
    if not _agent_state["search"]:
        return "ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        # Get video context for better search
        video_context = _agent_state.get("video_context", "")
        conversation_memory = _agent_state.get("conversation_memory", {})
        docs = _agent_state.get("docs", [])
        
        # Extract key terms from video content for better search context
        search_context_terms = []
        
        # Get video title
        if "Video Title: " in video_context:
            title = video_context.split("Video Title: ")[1].split("\n")[0]
            if title and title != "YouTube Video":
                search_context_terms.append(f'"{title}"')
        
        # Add memory context
        if conversation_memory:
            for key, value in conversation_memory.items():
                if key == "channel":
                    search_context_terms.append(f'"{value}"')
                elif key == "speaker":
                    search_context_terms.append(f'"{value}"')
                elif key == "event":
                    search_context_terms.append(f'"{value}"')
        
        # Extract key terms from video content (first 1000 characters)
        if docs and len(docs) > 0:
            content_sample = docs[0].page_content[:1000]
            # Use LLM to extract key terms for search
            key_terms_prompt = f"""ë‹¤ìŒ ë¹„ë””ì˜¤ ë‚´ìš©ì—ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. 
íšŒì‚¬ëª…, ì œí’ˆëª…, ê¸°ìˆ ëª…, ì¸ë¬¼ëª… ë“±ì„ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¹„ë””ì˜¤ ë‚´ìš©:
{content_sample}

í‚¤ì›Œë“œ:"""
            
            key_terms_response = _agent_state["llm"].invoke([HumanMessage(content=key_terms_prompt)])
            key_terms = key_terms_response.content.strip()
            if key_terms and len(key_terms) > 5:
                search_context_terms.append(key_terms)
        
        # Create enhanced search query
        if search_context_terms:
            enhanced_query = f"{query} {' '.join(search_context_terms)}"
        else:
            enhanced_query = query
            
        print(f"DEBUG: Original query: {query}")
        print(f"DEBUG: Enhanced search query: {enhanced_query}")
        
        search_result = _agent_state["search"].invoke(enhanced_query)
        
        # Filter and contextualize search result
        filter_prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ í˜„ì¬ ë¹„ë””ì˜¤ì™€ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ì„ ë³„í•˜ê³  í•œêµ­ì–´ë¡œ ë²ˆì—­/ìš”ì•½í•´ì£¼ì„¸ìš”.

í˜„ì¬ ë¹„ë””ì˜¤ ì •ë³´:
- ì œëª©: {video_context}
- ì‚¬ìš©ì ì œê³µ ì •ë³´: {json.dumps(conversation_memory, ensure_ascii=False)}
- ë¹„ë””ì˜¤ ì£¼ìš” ë‚´ìš©: {content_sample[:500] if docs and len(docs) > 0 else 'ì •ë³´ ì—†ìŒ'}

ê²€ìƒ‰ ê²°ê³¼:
{search_result}

ê´€ë ¨ì„±ì´ ë†’ì€ ë‚´ìš©ë§Œ ì„ ë³„í•˜ì—¬ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ê´€ë ¨ì„±ì´ ë‚®ì€ ë‚´ìš©ì€ ì œì™¸í•˜ê³ , 
í˜„ì¬ ë¹„ë””ì˜¤ì™€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ëœ ê¸°ì‚¬ë‚˜ ì •ë³´ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”:"""
        
        response = _agent_state["llm"].invoke([HumanMessage(content=filter_prompt)])
        return response.content
        
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"

@tool
def store_memory(user_message: str) -> str:
    """Stores factual information provided by the user about the video (e.g., channel name, speaker name).
    ONLY use when user provides NEW facts as STATEMENTS, NOT for questions."""
    try:
        extraction_prompt = f"""Analyze the following user message and extract any factual information they are providing about the video.
Return ONLY a JSON object with key-value pairs. If no factual information is provided, return {{}}.

Examples:
- "ì´ ë¹„ë””ì˜¤ëŠ” AWS Events ì±„ë„ì— ì˜¬ë¼ì™”ì–´" â†’ {{"channel": "AWS Events"}}
- "ë°œí‘œìëŠ” Johnì´ì•¼" â†’ {{"speaker": "John"}}
- "ì´ê±´ re:Invent 2024 ì„¸ì…˜ì´ì•¼" â†’ {{"event": "re:Invent 2024"}}

User message: {user_message}

Return JSON:"""
        
        response = _agent_state["llm"].invoke([HumanMessage(content=extraction_prompt)])
        content = response.content.strip()
        
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].strip()
        
        extracted_data = json.loads(content)
        
        if extracted_data:
            _agent_state["conversation_memory"].update(extracted_data)
            stored_keys = ", ".join(extracted_data.keys())
            return f"âœ“ ì •ë³´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {stored_keys}"
        else:
            return "ì €ì¥í•  ìƒˆë¡œìš´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        return f"ë©”ëª¨ë¦¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}"

@tool
def answer_question(question: str) -> str:
    """Answers specific questions about the video content using the transcript. 
    Use for detailed questions about what was said in the video."""
    if not _agent_state["qa_chain"]:
        return "ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ë¥¼ ë‹¤ì‹œ ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    try:
        # Create a Korean-specific QA prompt
        korean_qa_prompt = PromptTemplate(
            template="""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:""",
            input_variables=["context", "question"]
        )
        
        # Update QA chain with Korean prompt
        try:
            from langchain.chains import RetrievalQA
        except ImportError:
            from langchain_classic.chains import RetrievalQA
            
        qa_chain = RetrievalQA.from_chain_type(
            llm=_agent_state["llm"],
            chain_type="stuff",
            retriever=_agent_state["vector_store"].as_retriever(),
            chain_type_kwargs={"prompt": korean_qa_prompt}
        )
        
        return qa_chain.run(question)
    except Exception as e:
        return f"ì§ˆë¬¸ ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ============================================================================
# YOUTUBE AGENT CLASS
# ============================================================================

class YouTubeAgent:
    def __init__(self, openai_api_key):
        self.openai_api_key = openai_api_key
        os.environ["OPENAI_API_KEY"] = openai_api_key
        self.llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
        self.docs = []
        self.vector_store = None
        self.qa_chain = None
        self.search = DuckDuckGoSearchRun()
        self.video_context = ""
        self.conversation_memory = {}
        self.video_id = ""
        
        # Update global state
        _agent_state["llm"] = self.llm
        _agent_state["search"] = self.search
        
        # Create tools mapping for simple routing
        self.tools = {
            "summarize": summarize_video,
            "titles": generate_titles,
            "blog": write_blog_post,
            "quiz": generate_quiz,
            "moments": extract_key_moments,
            "translate": translate_content,
            "search": search_web,
            "memory": store_memory,
            "answer": answer_question
        }
    
    def load_video(self, url):
        """Loads the video transcript."""
        try:
            self.docs = []
            self.vector_store = None
            self.qa_chain = None
            self.video_context = ""

            loader = YoutubeLoader.from_youtube_url(url, add_video_info=False, language=["en", "en-US", "ko"])
            self.docs = loader.load()
            
            if not self.docs:
                return "ì˜¤ë¥˜: ì´ ë¹„ë””ì˜¤ì˜ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´ ìë§‰ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
                
            title = self.docs[0].metadata.get('title', 'YouTube Video')
            self.docs[0].metadata['title'] = title
            self.video_context = f"Video Title: {title}"
            
            # Extract video ID
            if "v=" in url:
                self.video_id = url.split("v=")[1].split("&")[0]
            elif "youtu.be/" in url:
                self.video_id = url.split("youtu.be/")[1].split("?")[0]
            else:
                import hashlib
                self.video_id = hashlib.md5(url.encode()).hexdigest()[:12]
            
            # Update global state
            _agent_state["docs"] = self.docs
            _agent_state["video_context"] = self.video_context
                
            return f"ë¹„ë””ì˜¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {title}"
        except Exception as e:
            return f"ë¹„ë””ì˜¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def create_vector_store(self):
        """Creates a FAISS vector store, using local persistence."""
        if not self.docs:
            return "ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        db_path = f"db/{self.video_id}"
        
        if os.path.exists(db_path):
            try:
                embeddings = OpenAIEmbeddings()
                self.vector_store = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
                print(f"DEBUG: Loaded existing vector store from {db_path}")
            except Exception as e:
                print(f"Warning: Could not load local index: {e}. Recreating.")
        
        if not self.vector_store:
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            split_docs = text_splitter.split_documents(self.docs)
            
            embeddings = OpenAIEmbeddings()
            self.vector_store = FAISS.from_documents(split_docs, embeddings)
            self.vector_store.save_local(db_path)
            print(f"DEBUG: Saved new vector store to {db_path}")
        
        # Create Korean QA chain
        korean_qa_prompt = PromptTemplate(
            template="""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:""",
            input_variables=["context", "question"]
        )
        
        try:
            from langchain.chains import RetrievalQA
        except ImportError:
            from langchain_classic.chains import RetrievalQA
            
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(),
            chain_type_kwargs={"prompt": korean_qa_prompt}
        )
        
        # Generate context summary in Korean
        if "Summary:" not in self.video_context:
            try:
                brief_content = self.docs[0].page_content[:3000]
                summary_prompt = f"ë‹¤ìŒ ë¹„ë””ì˜¤ ë‚´ìš©ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{brief_content}"
                context_summary = self.llm.invoke([HumanMessage(content=summary_prompt)]).content
                self.video_context += f"\nSummary: {context_summary}"
                print(f"DEBUG: Generated context summary")
            except Exception as e:
                print(f"Warning: Could not generate context summary: {e}")
        
        # Update global state
        _agent_state["vector_store"] = self.vector_store
        _agent_state["qa_chain"] = self.qa_chain
        _agent_state["video_context"] = self.video_context
        
        return "ë²¡í„° ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±/ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."

    def run(self, query):
        """Simple routing-based agent without complex agent framework."""
        if not self.docs:
            return "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        # Pre-check: answer from memory if applicable
        if self.conversation_memory and any(keyword in query.lower() for keyword in ['ë­', 'ë¬´ì—‡', 'ì–´ë–¤', 'ëˆ„êµ¬', 'ì–´ë””', 'ì–¸ì œ', 'what', 'who', 'which', 'where', 'when', 'ì±„ë„']):
            try:
                memory_check_prompt = f"""User has stored the following information:
{json.dumps(self.conversation_memory, ensure_ascii=False, indent=2)}

User question: {query}

If the question is asking about information that exists in the stored data above, answer it directly and concisely in Korean.
If the information is NOT in the stored data, respond with "NOT_FOUND".

Answer:"""
                
                response = self.llm.invoke([HumanMessage(content=memory_check_prompt)])
                answer = response.content.strip()
                
                if answer != "NOT_FOUND" and "NOT_FOUND" not in answer:
                    return f"ğŸ’¾ ì €ì¥ëœ ì •ë³´: {answer}"
            except Exception as e:
                print(f"Memory check error: {e}")
        
        # Update global state before processing
        _agent_state["conversation_memory"] = self.conversation_memory
        
        # Check if user is providing information (statements, not questions)
        if not any(q_word in query for q_word in ['?', 'ë­', 'ë¬´ì—‡', 'ì–´ë””', 'ëˆ„êµ¬', 'ì–¸ì œ', 'ì–´ë–»ê²Œ', 'what', 'who', 'where', 'when', 'how']):
            # This might be a statement providing information
            try:
                result = store_memory.invoke(query)
                if "ì •ë³´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤" in result:
                    # Update local memory as well
                    extraction_prompt = f"""Analyze the following user message and extract any factual information they are providing about the video.
Return ONLY a JSON object with key-value pairs. If no factual information is provided, return {{}}.

Examples:
- "ì´ ë¹„ë””ì˜¤ëŠ” AWS Events ì±„ë„ì— ì˜¬ë¼ì™”ì–´" â†’ {{"channel": "AWS Events"}}
- "ë°œí‘œìëŠ” Johnì´ì•¼" â†’ {{"speaker": "John"}}
- "ì´ê±´ re:Invent 2024 ì„¸ì…˜ì´ì•¼" â†’ {{"event": "re:Invent 2024"}}

User message: {query}

Return JSON:"""
                    
                    response = self.llm.invoke([HumanMessage(content=extraction_prompt)])
                    content = response.content.strip()
                    
                    if "```json" in content:
                        content = content.split("```json")[1].split("```")[0].strip()
                    elif "```" in content:
                        content = content.split("```")[1].strip()
                    
                    try:
                        extracted_data = json.loads(content)
                        if extracted_data:
                            self.conversation_memory.update(extracted_data)
                    except:
                        pass
                    
                    return result
            except Exception as e:
                print(f"Memory storage error: {e}")
        
        # Simple routing logic
        query_lower = query.lower()
        
        # Check for specific tool requests
        if any(word in query_lower for word in ['ìš”ì•½', 'summary', 'summarize']):
            return summarize_video.invoke("")
        elif any(word in query_lower for word in ['ì œëª©', 'title', 'titles']):
            return generate_titles.invoke("")
        elif any(word in query_lower for word in ['ë¸”ë¡œê·¸', 'blog', 'post']):
            return write_blog_post.invoke("")
        elif any(word in query_lower for word in ['í€´ì¦ˆ', 'quiz', 'test']):
            return generate_quiz.invoke("")
        elif any(word in query_lower for word in ['í•µì‹¬', 'ì¤‘ìš”', 'key', 'moments', 'highlights']):
            return extract_key_moments.invoke("")
        elif any(word in query_lower for word in ['ë²ˆì—­', 'translate', 'í•œêµ­ì–´']):
            return translate_content.invoke("Korean")
        elif any(word in query_lower for word in ['ê²€ìƒ‰', 'search', 'ì°¾ì•„', 'ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ê´€ë ¨', 'ì¶œì²˜']):
            # Don't modify the original query, just use it as is for search
            print(f"DEBUG: Search triggered by query: {query}")
            
            # If the query is asking for articles or sources, create a better search query
            if any(word in query_lower for word in ['ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ê´€ë ¨', 'ì¶œì²˜']):
                # Use the original query for search
                search_query = query
            else:
                # Extract search terms from query and enhance with video context
                search_query = query.replace('ê²€ìƒ‰', '').replace('ì°¾ì•„', '').replace('search', '').strip()
                
                # If no specific search terms after cleaning, use video context
                if not search_query or len(search_query) < 3:
                    if self.conversation_memory:
                        # Use stored information for search
                        search_terms = []
                        for key, value in self.conversation_memory.items():
                            search_terms.append(str(value))
                        search_query = " ".join(search_terms) + " ê´€ë ¨ ê¸°ì‚¬"
                    else:
                        # Use video title
                        title = self.docs[0].metadata.get('title', '') if self.docs else ''
                        search_query = f"{title} ê´€ë ¨ ê¸°ì‚¬" if title else "ê²€ìƒ‰í•  ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”."
            
            if search_query and search_query != "ê²€ìƒ‰í•  ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.":
                return search_web.invoke(search_query)
            else:
                return "ê²€ìƒ‰í•  ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”."
        else:
            # Default to answering questions about the video
            return answer_question.invoke(query)
